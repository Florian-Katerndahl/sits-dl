BASELINE: 53 minutes

Inductor together with reduce_overhead should already apply cuda-graph optimizations

Inductor backend:
default         compile             and amp with BS 512  without mask: 7,2 minutes for 1st chunk; ~3.4 minutes for subsequent chunks => would have been 39 minutes; guess: 27 minutes with mask
reduce-overhead compile             and amp with BS 512  without mask: 5,7 minutes for 1st chunk; ~3,4 minutes for subsequent chunks => guess: 37 minutes;          guess: 26 minutes with mask
reduce-overhead compile             and amp with BS 1024 without mask: did not finish compilation after 15 minutes....
max-autotune    compile             and amp with BS 512  without mask: does not compile, errors
reduce-overhead compile + fullgraph and amp with BS 1024 without mask: .... (possibly segfaults); segfaulted twice
reduce-overhead compile + fullgraph and amp with BS 512  without mask: 6.1 minutes for 1st chunk; ~3.5 minutes for subsequent chunks => 40 minutes
max-autotune    compule + fullgraph and amp with BS 512  without mask: 5.7 minutes for first chunk (was 9 minutes beforehand?!); ~3.2 minutes for subsequent chunkgs => 35 minutes
	- currently errors; either because of float32 usage or bad trition version (https://github.com/triton-lang/triton/issues/1556; https://github.com/triton-lang/triton/issues/1955#issuecomment-1929908209; https://github.com/pytorch/pytorch/issues/119054). Most likely, the latter one as the provided example from triton do not run as well.
	-> fixed by setting environment paths:
		- TRITON_NVDISASM_PATH=/usr/local/cuda-11.8/bin/nvdisasm
		- TRITON_CUOBJDUMP_PATH=/usr/local/cuda-11.8/bin/cuobjdump
		- TRITON_PTXAS_PATH=/usr/local/cuda-11.8/bin/ptxas
	-> no difference if cuda/ or cuda-11.8/; maybe the latter one is more robust? IDK but since on this VM I have 11.4, maybe simply using cuda suffices/is better...
	- the compiled model seems to get cached! Because re-running with BS 1024 prints optimization passes from Triton... => can I store the compiled model and load it afterwards? That would be wonderful; this would also explain why the first pass took 9 minutes and the subsequent only 5 for the first chunk, I guess
max-autotune    compile + fullgraph and amp with BS 1024 without mask: 12 minute for first chunk; ~4.1 minutes for subsequent chunks => 54 minutes total; 47 minutes on second run
max-autotune    compile + fullgraph and amp with BS 256  without mask: 5.1 minutes for first chunk; ~2.8 minutes for subsequent chunks => 30 minutes total on first run; 29 minutes on subsequent run
max-autotune    compile + fullgrpah and amp with BS 256  with mask:   cache overflow when using mask in first chunk; not every chunk can be of different size when using compiled models apparently... 

cudagraphs backend:
default         compile             and amp with BS 512  without mask: 13 minutes for 1st chunk; ~12 minutes for subsequent chunks => 125 minutes

TVM backend
default         compile             and amp with BS 512  without mask: installation of external tool requried; i.e. does not work

=> compilation does not work together with masks becaus it needs to re-compile for every new shape; thus for (almost) every new batch...
=> at least for X0057_Y0048 it seems that the best compiled model is approx. equal in run time as the non-conpiled model with mask
=> normalizing with pytorch speeds up things drastically but also increases memory usage to approx 40 GB?!
	=> GPU processing is only viable if enough VRAM is available. Alternatively, chunk size must be decreased
=> allow user to choose between plathora of different options: e.g. compiled, amp, preprocessing on GPU/CPU
=> do I have a memory leak? Smells like it